<!DOCTYPE html>
<html>
  <head>
    <title>Refining Pre-Trained Motion Models</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <!-- <link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'> -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <style>
      html,body,h1,h2,h3,h4,h5,h6 {font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;}
      <!-- .cite { background:#f0f0f0; padding:10px; font-size:18px} -->
      .cite { padding:0px; background:#ffffff; font-size:18px}
      .card {border: 1px solid #ccc}
      img { margin-bottom:-6px;}
      h1 { font-size:52px; font-weight:lighter}
      p { font-size:18px;}
      a {text-decoration: none; color: #2196F3;}
      .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
      0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
      5px 5px 0 0px #fff, /* The second layer */
      5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
      10px 10px 0 0px #fff, /* The third layer */
      10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
      15px 15px 0 0px #fff, /* The fourth layer */
      15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
      20px 20px 0 0px #fff, /* The fifth layer */
      20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
      25px 25px 0 0px #fff, /* The fifth layer */
      25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
      margin-left: 10px;
      margin-right: 60px;
      }
      .videoWrapper {
      position: relative;
      padding-bottom: 56.25%;
      /* 16:9 */
      padding-top: 25px;
      height: 0;
      }
      .videoWrapper iframe {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      }
      
    </style>
  </head>  
  <body class="w3-white">
    <!-- Page Container -->
    <div class="w3-content w3-margin-top w3-margin-bottom" style="max-width:960px;">


      <!-- The Grid -->
      <div class="w3-row-padding">

	<!-- paper container -->	  
	<div class="w3-display-container w3-row w3-white w3-margin-bottom">
	  <div class="w3-center">
	    <h1>Refining Pre-Trained Motion Models</h1>
	    <h5> <a href="https://xinglongsun.com/">Xinglong Sun</a> &emsp;&emsp; <a href="https://adamharley.com/">Adam W. Harley</a> &emsp;&emsp;  <a href="https://geometry.stanford.edu/member/guibas/">Leonidas J. Guibas</a></h5>
	    <!-- <h5><em>ICRA 2023</em></h5> -->
	  </div>
	  <div class="w3-center">
	    <h3>[<a href="https://github.com/AlexSunNik/refining-motion-code">Code</a>] &emsp; [<a href="https://arxiv.org/abs/2401.00850">Paper</a>]</h3>
	    <!-- <h3>[<a href="https://github.com/aharley/simple_bev">Code</a>] &emsp; [<a href="simple_bev_sep30.pdf">Paper</a>] &emsp; [<a href="https://arxiv.org/abs/2206.07959">arXiv</a>]</h3> -->
	    <!-- <h3>[<a href="https://github.com/aharley/simple_bev">Code</a>]</h3> -->
	  </div>
	  <hr>

	  
	  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
	    <!-- <h4>5 minute overview</h4> -->
	    <div class="videoWrapper">
	      <iframe width="560" height="349" src="https://www.youtube.com/embed/fJsTEBrTjVc" frameborder="0" allowfullscreen></iframe>
	    </div>
	  </div>
	  <hr>
	  
	  <div class="w3-center">
	    <h2>Abstract</h2>
	  </div>
	  <p>Given the difficulty of manually annotating motion in video, the current best motion estimation methods are trained with synthetic data, and therefore struggle somewhat due to a train/test gap. Self-supervised methods hold the promise of training directly on real video, but typically perform worse. These include methods trained with warp error (i.e., color constancy) combined with smoothness terms, and methods that encourage cycle-consistency in the estimates (i.e., tracking backwards should yield the opposite trajectory as tracking forwards). In this work, we take on the challenge of improving state-of-the-art supervised models with self-supervised training. We find that when the initialization is supervised weights, most existing self-supervision techniques actually make performance worse instead of better, which suggests that the benefit of seeing the new data is overshadowed by the noise in the training signal. Focusing on obtaining a "clean" training signal from real-world unlabelled video, we propose to separate label-making and training into two distinct stages. In the first stage, we use the pre-trained model to estimate motion in a video, and then select the subset of motion estimates which we can verify with cycle-consistency. This produces a sparse but accurate pseudo-labelling of the video. In the second stage, we fine-tune the model to reproduce these outputs, while also applying augmentations on the input. We complement this boot-strapping method with simple techniques that densify and re-balance the pseudo-labels, ensuring that we do not merely train on "easy" tracks. We show that our method yields reliable gains over fully-supervised methods in real videos, for both short-term (flow-based) and long-range (multi-frame) pixel tracking.</p>
	  <hr>
	  <div class="w3-center">
	    <h2>Method Overview</h2>
	  </div>
	  
	  <div class="w3-center">
	    <div class="w3-card">
	      <!-- <div class="w3-padding"><h4>RGB-only model</h4></div> -->
	      <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
		<h4>Step 1. Generate Motion Estimates</h4>

		<!-- <img src="images/bev_vis.png" style="width:100%"> -->
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/m1a.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
		<p>We first run the pretrained motion model on a unlabeled video to generate some motion estimates.</p>
			</div>
	  </div>
	  <br />
	  
	  <div class="w3-center">
	    <div class="w3-card">
	      <!-- <div class="w3-padding"><h4>RGB-only model</h4></div> -->
	      <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
		<h4>Step 2. Filter Estimates to Obtain Pseudo Labels</h4>

		<!-- <img src="images/bev_vis.png" style="width:100%"> -->
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/m2a.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
		<p>Then we design a cycle-consistency based filter to filter those estimates down to "good" and "clean" trajectories. Specifically, cycle consistency assumes that if for a good track, if we track forwards in time, then start at the endpoint and track backwards in time, we should end up at the original startpoint.</p>
			</div>
	  </div>
	  <br />

	  <div class="w3-center">
	    <div class="w3-card">
	      <!-- <div class="w3-padding"><h4>RGB-only model</h4></div> -->
	      <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
		<h4>Step 3. Finetuning the Pretrained Model </h4>

		<!-- <img src="images/bev_vis.png" style="width:100%"> -->
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/m3a.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
		<p>In the next stage, we perform some augmentations on those good estimates, and finetune the same pretrained model to reproduce those estimations.</p>
			</div>
	  </div>
	  <br />

	  <hr>
	  <div class="w3-center">
	    <h2>Results</h2>
	  </div>
	  <p>Please see the paper for detailed quantitative results and analysis. Here, we show some visualizations of the performance.</p>
	  
	  <div class="w3-center">
	    <h3>Optical Flow</h3>
	  </div>
	  
	  <div class="w3-card w3-padding">
		<img src="images/raft2.png" style="width:100%">
	  </div>
	  <p> Percent change in EPE after our refinement for each video in the MPI Sintel dataset compared with the pretrained RAFT model. Negative value denotes improvement (i.e., error decreasing)</p>

	  <div class="w3-card w3-padding">
		<img src="images/draw1_correct.png" style="width:100%">
	  </div>

	  <div class="w3-card w3-padding">
		<img src="images/draw2_correct.png" style="width:100%">
	  </div>

	  <div class="w3-card w3-padding">
		<img src="images/draw3_correct.png" style="width:100%">
	  </div>
	  
	  <p>Our approach refines the flows by completing missing objects and cleaning noisy tracks.</p>

	  <div class="w3-center">
	    <h3>Multi-frame Tracking</h3>
	  </div>

	  <div class="w3-card w3-padding">
		<img src="images/davis_final.png" style="width:100%">
	  </div>
	  <p> Performance change in Î´ after our refinement for each of the video in the TapVid-DAVIS dataset compared with the baseline pretrained PIPs model. Positive value denotes improvement (i.e., accuracy increasing)</p>


	  <div class="w3-center">
	    <div class="w3-card">
	      <!-- <div class="w3-padding"><h4>RGB-only model</h4></div> -->
	      <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
		<!-- <h4>Step 3. Finetuning the Pretrained Model </h4> -->

		<!-- <img src="images/bev_vis.png" style="width:100%"> -->
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/r1a.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
		<p>Tracking a point on the background, original tracker is stuck on the moving car whereas our refinement fixes the error.</p>
			</div>
	  </div>
	  <br />

	  <div class="w3-center">
	    <div class="w3-card">
	      <!-- <div class="w3-padding"><h4>RGB-only model</h4></div> -->
	      <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
		<!-- <h4>Step 3. Finetuning the Pretrained Model </h4> -->

		<!-- <img src="images/bev_vis.png" style="width:100%"> -->
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/r2a.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
		<p>Tracking a point on the dancer's leg, original tracker loses the target in the middle whereas our refinement helps to keep the trajectory to the end.</p>
			</div>
	  </div>
	  <br />


	  <div class="w3-center">
	    <div class="w3-card">
	      <!-- <div class="w3-padding"><h4>RGB-only model</h4></div> -->
	      <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
		<!-- <h4>Step 3. Finetuning the Pretrained Model </h4> -->

		<!-- <img src="images/bev_vis.png" style="width:100%"> -->
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/r3a.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
		<p>Tracking a point on the dog, original tracker is stuck on the standing person whereas our refinment corrects this mistake.</p>
			</div>
	  </div>
	  <br />

	  <hr>

	  <div class="w3-row w3-margin" style="padding-bottom:2em">
	    <div class="w3-center"><h2>Paper</h2></div>
	    <div class="w3-col s0 m1 l2" style="height:10px"></div>
	    <div class="w3-col s6 m3 l2">
	      <a href="https://arxiv.org/abs/2401.00850"><img class="layered-paper-big" src="images/page1.png" style="width:100%;min-height:200px; margin-right:3em"></a>
	    </div>
	    <div class="w3-col s6 m7 l6" style="padding-left:5em">
	      <div class="cite">
			Xinglong Sun, Adam W. Harley, Leonidas J. Guibas.
	  	<i>Refining Pre-Trained Motion Models</i>
	  	<!-- IEEE International Conference on Robotics and Automation (ICRA) 2023. -->
	      </div>
	      <h3><a href="https://arxiv.org/abs/2401.00850">[abs]</a>&emsp;<a href="https://arxiv.org/pdf/2401.00850.pdf">[pdf]</a>&emsp;<a href="bib.txt">[bibtex]</a></h3>
	      <!-- <h3><a href="https://arxiv.org/abs/2206.07959">[abs]</a>&emsp;<a href="https://arxiv.org/pdf/2206.07959.pdf">[pdf]</a></h3> -->
	    </div>
	    <div class="w3-col s0 m1 l2" style="height:10px"></div>

	  </div>

	  <hr>
	  
	  
	  <div class="w3-row w3-margin" style="padding-bottom:2em">
	    <!-- <h2><a href="bib.txt">[bibtex]</a></h3> -->
	    <div class="w3-center"><h2>Bibtex</h2></div>
	      <div class="w3-code notranslate">
	  	@inproceedings{sun2024refining,<br>
	  	&emsp;author    = {Xinglong Sun and Adam W. Harley and Leonidas J. Guibas}, <br>
	  	&emsp;title     = {Refining Pre-Trained Motion Models},<br>
	  	&emsp;year      = {2024},<br>
		&emsp;eprint    = {2401.00850},<br>
		&emsp;archivePrefix =  {arXiv},
	  	}
	      </div>
	    
	  </div>

	  
	  <!-- <hr> -->

	  <!-- <div class="w3-center"><h2>Bibtex</h2></div> -->

	  
	  <!-- end paper container -->


	</div><!-- End Grid -->
      </div><!-- End Page Container -->

  </body>
</html>